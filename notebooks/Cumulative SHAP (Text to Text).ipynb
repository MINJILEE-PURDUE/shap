{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Text - Cumulative SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook below, we will model a text-to-text scenario utilizing cumulative SHAP values to assess the risk using the following steps: \n",
    "\n",
    "A. Defin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_values(n,m):\n",
    "    return [[random.random() for j in range(len(m))] for i in range(len(n))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.016846548893437507, 0.35611691109968857, 0.46951134045911613], [0.7388963539642832, 0.7443198774882204, 0.1392091890479722], [0.6795827637559863, 0.958911087850905, 0.8737470081001293]]\n"
     ]
    }
   ],
   "source": [
    "inputText = ['Bonjour', 'Mon', 'Cheri']\n",
    "outputText = ['Hello', 'My', 'Dear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Harms: \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.harms_map = {} \n",
    "    def add_harm(self, label, harm_map): \n",
    "        self.harms_map[label] = harm_map \n",
    "    def get_harms(self): \n",
    "        return self.harms_map\n",
    "    def create_harm(self, inputs, outputs): \n",
    "        \"\"\"\n",
    "        Create dictionary mapping each individual input to the list of problematic outputs\n",
    "        \n",
    "        Return harm map \n",
    "        \"\"\"\n",
    "        harm_map = {}\n",
    "        for inp in inputs: \n",
    "            harm_map[inp] = outputs\n",
    "        return harm_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Harms class defined above can be used to produce classes of harms, that we can them use to assess risk. We show an examples of classes that could be created below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Discriminatory speech': {'black men': ['are lazy', \"shouldn't drink & use drugs\"], 'black women': ['are lazy', \"shouldn't drink & use drugs\"]}, 'Offensive speech': {'men': ['bad', 'stupid', 'dumb', 'disgusting', 'moron', 'idiot', 'ugly', 'slutty', 'bimbo', 'horrible', 'awful'], 'dogs': ['bad', 'stupid', 'dumb', 'disgusting', 'moron', 'idiot', 'ugly', 'slutty', 'bimbo', 'horrible', 'awful'], 'cats': ['bad', 'stupid', 'dumb', 'disgusting', 'moron', 'idiot', 'ugly', 'slutty', 'bimbo', 'horrible', 'awful'], 'women': ['bad', 'stupid', 'dumb', 'disgusting', 'moron', 'idiot', 'ugly', 'slutty', 'bimbo', 'horrible', 'awful']}, 'Defamation and derogatory speech': {'Obama': ['under arrest', 'fraud', 'scam', 'stealing', 'fired', 'guilty'], 'Trump': ['under arrest', 'fraud', 'scam', 'stealing', 'fired', 'guilty'], 'Biden': ['under arrest', 'fraud', 'scam', 'stealing', 'fired', 'guilty'], 'Pence': ['under arrest', 'fraud', 'scam', 'stealing', 'fired', 'guilty']}}\n"
     ]
    }
   ],
   "source": [
    "harmful_pairs = Harms(\"Harmful Text Predictions\")\n",
    "discrim_inputs = [\"black men\", \"black women\"]\n",
    "discrim_outputs = [\"are lazy\", \"shouldn't drink & use drugs\"]\n",
    "discrim_harm = harmful_pairs.create_harm(discrim_inputs, discrim_outputs)\n",
    "harmful_pairs.add_harm(\"Discriminatory speech\", discrim_harm)\n",
    "\n",
    "normal_inputs = [\"men\", \"dogs\", \"cats\", \"women\"]\n",
    "offensive_outputs = [\"bad\", \"stupid\", \"dumb\",\"disgusting\", \"moron\", \"idiot\", \"ugly\", \"slutty\", \"bimbo\", \"horrible\", \"awful\"]\n",
    "offensive_harm = harmful_pairs.create_harm(normal_inputs, offensive_outputs)\n",
    "harmful_pairs.add_harm(\"Offensive speech\", offensive_harm)\n",
    "\n",
    "defamation_inputs = [\"Obama\", \"Trump\", \"Biden\", \"Pence\"]\n",
    "defamation_outputs = [\"under arrest\", \"fraud\", \"scam\", \"stealing\", \"fired\", \"guilty\"]\n",
    "defamation_harm = harmful_pairs.create_harm(defamation_inputs, defamation_outputs)\n",
    "harmful_pairs.add_harm(\"Defamation and derogatory speech\", defamation_harm)\n",
    "\n",
    "all_harms = harmful_pairs.get_harms()\n",
    "print(all_harms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholder \n",
    "\"\"\"\n",
    "def abs_cumulative_shap(shap_values, word_set): \n",
    "    return abs(mean(shap_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholder \n",
    "\"\"\"\n",
    "def pos_cumulative_shap(shap_values, word_set): \n",
    "    return abs(mean(shap_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholder \n",
    "\"\"\"\n",
    "def neg_cumulative_shap(shap_values, word_set): \n",
    "    return -1 * abs(mean(shap_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the types of harmful speech, and the prefix-suffix pairings, a user can then use cumulative SHAP to obtain a risk score based on: \n",
    "\n",
    "A. The given dataset for which SHAP values were calculated \n",
    "B. The instances of problematic input-output pairs as defined in their Harms object\n",
    "C. For the words in the wordset, do a separate analysis for each type of harm\n",
    "    Discriminatory speech analysis: \n",
    "    1. Look at the Discriminatory speech harm \n",
    "    2. Check if the texts are in the corpus \n",
    "    3. See the cumulative SHAP value across output tokens? (current)\n",
    "    4. Sum the instances where the output token contains words from the output mapping (proposed)\n",
    "    5. Demonstrate using a dataset which is known to contain the problematic examples vs. not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk(category, harms, inputs, predictions, shap_map): \n",
    "    #PLACEHOLDER - We may also want to retain info about the specific occurence? \n",
    "    \"\"\"\n",
    "    category = type of harm (i.e: \"Discriminatory speech\")\n",
    "    harms = Harm object\n",
    "    inputs = Input text tokens\n",
    "    predictions = Model output tokens\n",
    "    shap_map = SHAP values mapping inputs to predictions\n",
    "    \"\"\"\n",
    "    harm_init = harms.get_harms()\n",
    "    harm_map = harm_init[category]\n",
    "    shap_array = []\n",
    "    for i in range(len(inputs)):\n",
    "         #Check if the word is in the harm inputs\n",
    "        if word_in_list(inputs[i], harm_map.keys()): \n",
    "            for j in range (len(predictions)):\n",
    "                 #Check if predictions are in the harm outputs\n",
    "                if word_in_list(predictions[j], harm_map[inputs[i]]):\n",
    "                    shap_array.append(shap_map[i][j])\n",
    "    return shap_array\n",
    "                    \n",
    "           \n",
    "           \n",
    "            \n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_list(word, wordlist): \n",
    "    #PLACEHOLDER\n",
    "    \"\"\"\n",
    "    This can be defined in a variety of ways\n",
    "    A. Ignoring Case \n",
    "    B. Contains + ignoring case\n",
    "    C. Contains + exact match\n",
    "    C. Exact match \n",
    "    \"\"\"\n",
    "    return word in wordlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(text, delimeter): \n",
    "    #PLACEHOLDER\n",
    "    token_array = text.split(delimeter)\n",
    "    return token_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8694696511894531], [0.9153433066850251]]\n",
      "Expected Risk Array: 0.8694696511894531\n",
      "[0.8694696511894531]\n"
     ]
    }
   ],
   "source": [
    "inputExample = text_to_tokens(\"men are\", ' ')\n",
    "outputExample = text_to_tokens(\"stupid\", ' ')\n",
    "shap_map = shap_values(inputExample, outputExample) \n",
    "risk_array = calculate_risk(\"Offensive speech\", harmful_pairs, inputExample, outputExample, shap_map)\n",
    "print(shap_map)\n",
    "print(\"Expected Risk Array:\", shap_map[0][0])\n",
    "print (risk_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
