{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed comparison of gradient boosting libraries for shap values calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare CatBoost, LightGBM and XGBoost for shap values calculations. All boosting algorithms were trained on GPU but shap evaluation was on CPU.\n",
    "\n",
    "We use the epsilon_normalized dataset from [here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import copy\n",
    "import tqdm\n",
    "import datetime\n",
    "from sklearn import datasets\n",
    "import catboost\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost.__version__, lgb.__version__, xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target = datasets.load_svmlight_file(\"epsilon_normalized\")\n",
    "test_data, test_target = datasets.load_svmlight_file(\"epsilon_normalized.t\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "lr = 0.1\n",
    "max_bin = 128\n",
    "gpu_device = '0' # specify your GPU (used only for training)\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target[train_target == -1] = 0\n",
    "test_target[test_target == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, label=None, mode='train', boosting=None):\n",
    "    assert boosting is not None\n",
    "    \n",
    "    if boosting == 'xgboost':\n",
    "        return xgb.DMatrix(data, label)\n",
    "    elif boosting == 'lightgbm':\n",
    "        if mode == 'train':\n",
    "            return lgb.Dataset(data, label)\n",
    "        else:\n",
    "            return data\n",
    "    elif boosting == 'catboost':\n",
    "        data = catboost.FeaturesData(num_feature_data=data)\n",
    "        return catboost.Pool(data, label)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown boosting library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(base_params, boosting=None, **kwargs):\n",
    "    assert boosting is not None\n",
    "    assert isinstance(base_params, dict)\n",
    "    \n",
    "    params = copy.copy(base_params)\n",
    "    if boosting == 'xgboost':\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['max_depth'] = kwargs['depth']\n",
    "        params['tree_method'] = 'gpu_hist'\n",
    "        params['gpu_id'] = gpu_device\n",
    "    elif boosting == 'lightgbm':\n",
    "        params['objective'] = 'binary'\n",
    "        params['device'] = \"gpu\"\n",
    "        params['gpu_device_id'] = gpu_device\n",
    "        params['num_leaves'] = 2**kwargs['depth']\n",
    "    elif boosting == 'catboost':\n",
    "        params['objective'] = 'Logloss'\n",
    "        params['task_type'] = 'GPU'\n",
    "        params['devices'] = gpu_device\n",
    "        params['bootstrap_type'] = 'Bernoulli'\n",
    "        params['logging_level'] = 'Silent'\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown boosting library\")\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, params, num_iters, boosting=None):\n",
    "    assert boosting is not None\n",
    "    if boosting == 'xgboost':\n",
    "        return xgb.train(params=params, dtrain=data, num_boost_round=num_iters)\n",
    "    elif boosting == 'lightgbm':\n",
    "        return lgb.train(params=params, train_set=data, num_boost_round=num_iters)\n",
    "    elif boosting == 'catboost':\n",
    "        return catboost.train(pool=data, params=params, num_boost_round=num_iters)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown boosting library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_shap(model, data, boosting=None):\n",
    "    assert boosting is not None\n",
    "    if boosting == 'xgboost':\n",
    "        return model.predict(data, pred_contribs=True)\n",
    "    elif boosting == 'lightgbm':\n",
    "        return model.predict(data, pred_contrib=True)\n",
    "    elif boosting == 'catboost':\n",
    "        return model.get_feature_importance(data, fstr_type='ShapValues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(boosting, params):\n",
    "    fname = [boosting]\n",
    "    for key, value in sorted(params.items()):\n",
    "        fname.append(str(key))\n",
    "        fname.append(str(value))\n",
    "    fname = \"_\".join(fname)\n",
    "    fname = fname.replace(\".\", '')\n",
    "    fname += \".model\"\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(fname, boosting):\n",
    "    if boosting == \"xgboost\":\n",
    "        bst = xgb.Booster(model_file=fname)\n",
    "        bst.load_model(fname)\n",
    "    elif boosting == \"lightgbm\":\n",
    "        bst = lgb.Booster(model_file=fname)\n",
    "    elif boosting == \"catboost\":\n",
    "        bst = catboost.CatBoost()\n",
    "        bst.load_model(fname)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown boosting\")\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {\n",
    "    'learning_rate': lr,\n",
    "    'max_bin': max_bin,\n",
    "    'random_state': random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "boosting_list = ['xgboost', 'catboost', 'lightgbm']\n",
    "depth_list = [2, 4, 6, 8, 10]\n",
    "lens_list = [1000, 5000, 10000]\n",
    "\n",
    "\n",
    "for gb_type in boosting_list:\n",
    "    \n",
    "    print(\"{} is going\".format(gb_type))\n",
    "    \n",
    "    for size_test in lens_list:\n",
    "        print(\"size test {}\".format(size_test))\n",
    "        sep_test_data = test_data[:size_test]\n",
    "        sep_test_target = test_target[:size_test]\n",
    "        \n",
    "        # comment this line if you have already trained all models\n",
    "        train_preprocessed = preprocess_data(train_data, train_target, boosting=gb_type)\n",
    "        \n",
    "        dense_test = sep_test_data.todense().A.astype(np.float32)\n",
    "        \n",
    "        for depth in tqdm.tqdm(depth_list):\n",
    "        \n",
    "            start_test_preproc = datetime.datetime.now()\n",
    "            test_preprocessed = preprocess_data(dense_test,\n",
    "                                                sep_test_target, \n",
    "                                                mode='test',\n",
    "                                                boosting=gb_type)\n",
    "        \n",
    "            finish_test_preproc = datetime.datetime.now()\n",
    "            preprocessing_delta = finish_test_preproc - start_test_preproc\n",
    "            preprocessing_delta = preprocessing_delta.total_seconds()\n",
    "\n",
    "            params = create_parameters(base_params, boosting=gb_type, depth=depth)\n",
    "            params['depth'] = depth\n",
    "            fname = create_path(gb_type, params)\n",
    "            if os.path.exists(fname):\n",
    "                print(\"model exist\")\n",
    "                bst = load_model(fname, boosting=gb_type)\n",
    "            else:\n",
    "                print(\"model is training\")\n",
    "                start_train = datetime.datetime.now()\n",
    "                bst = train(train_preprocessed, params, num_iters=num_iters, boosting=gb_type)\n",
    "                finish_train = datetime.datetime.now()\n",
    "                delta_train = finish_train - start_train\n",
    "                delta_train = int(delta_train.total_seconds() * 1000)\n",
    "                bst.save_model(fname)\n",
    "\n",
    "            start_time = datetime.datetime.now()\n",
    "            preds = predict_shap(bst, test_preprocessed, boosting=gb_type)\n",
    "            assert preds.shape == (sep_test_data.shape[0], sep_test_data.shape[1] + 1)\n",
    "            finish_time = datetime.datetime.now()\n",
    "\n",
    "            delta = finish_time - start_time\n",
    "            delta = delta.total_seconds()\n",
    "\n",
    "            current_res = {\n",
    "            'preprocessing_time': preprocessing_delta,\n",
    "            'boosting': gb_type,\n",
    "            'test_size': size_test,\n",
    "            'depth': depth,\n",
    "            'time': delta,\n",
    "            }\n",
    "\n",
    "            result.append(current_res)\n",
    "\n",
    "        print(\"*\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"shap_benchmark_{}_max_bin_with_test_sizes.csv\".format(max_bin), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(\"shap_benchmark_128_max_bin_with_test_sizes.csv\", )\n",
    "result_df.pivot_table(index=[\"test_size\", \"depth\"], columns=\"boosting\", values=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.pivot_table(index=\"test_size\", columns=\"boosting\", values=\"preprocessing_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
