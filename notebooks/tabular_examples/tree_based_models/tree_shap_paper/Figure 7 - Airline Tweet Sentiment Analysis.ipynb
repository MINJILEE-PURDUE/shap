{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 7 - Airline Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import shap\n",
    "import scipy as sp\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tweet data\n",
    "\n",
    "This is from http://kaggle.com/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/tweets.csv\")\n",
    "y = np.zeros(raw_data.shape[0])\n",
    "mask = raw_data[\"airline_sentiment\"] == \"positive\"\n",
    "y[mask] = 1 * raw_data[\"airline_sentiment_confidence\"][mask]\n",
    "mask = raw_data[\"airline_sentiment\"] == \"negative\"\n",
    "y[mask] = -1 * raw_data[\"airline_sentiment_confidence\"][mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "ps = PorterStemmer()\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "for i in range(raw_data.shape[0]):\n",
    "    words = raw_data[\"text\"][i].split()\n",
    "    words = [ps.stem(w.lower().translate(translate_table)) for w in words]\n",
    "    for i in range(len(words)):\n",
    "        word_counts[words[i]] = word_counts.get(words[i],0)+1\n",
    "\n",
    "word_inds = {}\n",
    "ind = 0\n",
    "for k in word_counts:\n",
    "    if word_counts[k] >= 10:\n",
    "        word_inds[k] = ind\n",
    "        ind += 1\n",
    "        \n",
    "X_npy = np.zeros((raw_data.shape[0], len(word_inds)))\n",
    "word_counts = {}\n",
    "for i in range(raw_data.shape[0]):\n",
    "    words = raw_data[\"text\"][i].split()\n",
    "    words = [ps.stem(w.lower().translate(translate_table)) for w in words]\n",
    "    for j in range(len(words)):\n",
    "        if words[j] in word_inds:\n",
    "            X_npy[i,word_inds[words[j]]] += 1\n",
    "            \n",
    "names = [\"\" for i in range(len(word_inds))]\n",
    "for k in word_inds:\n",
    "    names[word_inds[k]] = k\n",
    "X = pd.DataFrame(data=X_npy, columns=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a complete dataset\n",
    "xgb_full = xgboost.DMatrix(X, label=y)\n",
    "\n",
    "# create a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "xgb_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgboost.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use validation set to choose # of trees\n",
    "params = {\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 30,\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"base_score\": y_train.mean()\n",
    "}\n",
    "model_train = xgboost.train(params, xgb_train, 50, evals = [(xgb_test, \"test\")], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute differences in prediction when perturbing the highest weight feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_orig = []\n",
    "preds_shap_mod = []\n",
    "preds_saabas_mod = []\n",
    "preds_gain_mod = []\n",
    "L = X_test.shape[0]\n",
    "shap_sums = [0]\n",
    "saabas_sums = [0]\n",
    "gain_sums = [0]\n",
    "N = X_test.shape[0]\n",
    "rand_inds = np.array([np.random.randint(N) for i in range(N)])\n",
    "\n",
    "X_test_shuffle = X_test.iloc[rand_inds,:].copy()\n",
    "preds_orig = model_train.predict(xgboost.DMatrix(X_test))\n",
    "\n",
    "# Tree SHAP\n",
    "tmp = X_test.copy()\n",
    "shap_values = model_train.predict(xgboost.DMatrix(tmp), pred_contribs=True)\n",
    "inds = np.argmin(shap_values[:,:-1],1)\n",
    "for i in range(N):\n",
    "    tmp.iloc[i,inds[i]] = X_test_shuffle.iloc[i,inds[i]]\n",
    "preds_shap_mod = model_train.predict(xgboost.DMatrix(tmp))\n",
    "\n",
    "# Saabas\n",
    "tmp = X_test.copy()\n",
    "saabas_values = model_train.predict(xgboost.DMatrix(tmp), pred_contribs=True, approx_contribs=True)\n",
    "inds = np.argmin(saabas_values[:,:-1],1)\n",
    "for i in range(N):\n",
    "    tmp.iloc[i,inds[i]] = X_test_shuffle.iloc[i,inds[i]]\n",
    "preds_saabas_mod = model_train.predict(xgboost.DMatrix(tmp))\n",
    "\n",
    "# gain (multiplied by split count to get total gain instead of average gain)\n",
    "d = model_train.get_score(importance_type=\"gain\")\n",
    "wd = model_train.get_score(importance_type=\"weight\")\n",
    "keys = list(d.keys())\n",
    "vals = [d[k]*wd[k] for k in keys]\n",
    "gain_ind = np.argmax(vals)\n",
    "gain_ind = np.where(X_test.columns == keys[gain_ind])[0][0]\n",
    "print(\"gain chose\", X_test.columns[gain_ind])\n",
    "tmp = X_test.copy()\n",
    "for i in range(N):\n",
    "    if tmp.iloc[i,gain_ind] == 0:\n",
    "        tmp.iloc[i,gain_ind] = X_test_shuffle.iloc[i,gain_ind]\n",
    "preds_gain_mod = model_train.predict(xgboost.DMatrix(tmp))\n",
    "\n",
    "# split count\n",
    "wd = model_train.get_score(importance_type=\"weight\")\n",
    "keys = list(d.keys())\n",
    "vals = [wd[k] for k in keys]\n",
    "weight_ind = np.argmax(vals)\n",
    "weight_ind = np.where(X_test.columns == keys[weight_ind])[0][0]\n",
    "print(\"weight chose\", X_test.columns[weight_ind])\n",
    "tmp = X_test.copy()\n",
    "for i in range(N):\n",
    "    if tmp.iloc[i,weight_ind] == 1:\n",
    "        tmp.iloc[i,weight_ind] = X_test_shuffle.iloc[i,weight_ind]\n",
    "preds_weight_mod = model_train.predict(xgboost.DMatrix(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most important feature based on permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_vals = []\n",
    "for i in tqdm.tqdm(range(X_test.shape[1])):\n",
    "    tmp = X_test.copy()\n",
    "    tmp.iloc[:,i] = X_test_shuffle[:,i]\n",
    "    preds_tmp = model_train.predict(xgboost.DMatrix(tmp))\n",
    "    perm_vals.append(np.sqrt(np.mean((y_test-preds_tmp)**2)))\n",
    "print(\"permutation chose\", X_test.columns[np.argmax(perm_vals)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saabas_sums = [0]\n",
    "shap_sums = [0]\n",
    "gain_sums = [0]\n",
    "weight_sums = [0]\n",
    "for i in range(N):\n",
    "    saabas_sums.append(saabas_sums[-1]+preds_saabas_mod[i]-preds_orig[i])\n",
    "    shap_sums.append(shap_sums[-1]+preds_shap_mod[i]-preds_orig[i])\n",
    "    gain_sums.append(gain_sums[-1]+preds_gain_mod[i]-preds_orig[i])\n",
    "    weight_sums.append(weight_sums[-1]+preds_weight_mod[i]-preds_orig[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the perturbation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(shap_sums, label=\"SHAP\", color=(30/255, 136/255, 229/255), linewidth=3)\n",
    "pl.plot(saabas_sums, label=\"Saabas\", color=(245/255, 39/255, 87/255), linewidth=3)\n",
    "pl.plot(gain_sums, label=\"Gain\", color=(24/255, 196/255, 93/255), linewidth=3)\n",
    "pl.plot(gain_sums, label=\"Permutation\", color=(24/255, 196/255, 93/255), linewidth=3)\n",
    "pl.plot(weight_sums, label=\"Split Count\", color=(124/255, 82/255, 255/255), linewidth=3)\n",
    "pl.legend(frameon=False, fontsize=14)\n",
    "pl.ylabel(\"Total increase in model predictions\", fontsize=14)\n",
    "pl.xlabel(\"Samples (tweets)\", fontsize=14)\n",
    "pl.gca().tick_params(axis='x', labelsize=12)\n",
    "pl.gca().tick_params(axis='y', labelsize=12)\n",
    "#pl.savefig(\"data/sentiment_perturbation.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=10, show=False, color_bar=False)\n",
    "cb = pl.colorbar(ticks=[0,1], aspect=1000)\n",
    "cb.set_ticklabels([\"0\", \"> 0\"])\n",
    "cb.set_label(\"Word count\", size=12, labelpad=0)\n",
    "cb.ax.tick_params(labelsize=11, length=0)\n",
    "cb.set_alpha(1)\n",
    "cb.outline.set_visible(False)\n",
    "bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())\n",
    "cb.ax.set_aspect((bbox.height-0.9)*20)\n",
    "#pl.savefig(\"data/sentiment_summary.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "shap_pca50 = PCA(n_components=50).fit_transform(shap_values[:,:-1])\n",
    "shap_embedded = TSNE(n_components=2, perplexity=50).fit_transform(shap_pca50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "shap_pca2 = PCA(n_components=2).fit_transform(shap_values[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(5,5))\n",
    "pl.scatter(shap_pca2[:,0], shap_pca2[:,1], c=np.sum(shap_values,axis=1), linewidth=0, alpha=0.5, cmap=shap.plots.red_blue)\n",
    "cb = pl.colorbar(label=\"Model output\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "pl.gca().axis(\"off\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(5,5))\n",
    "pl.scatter(shap_embedded[:,0], shap_embedded[:,1], c=np.sum(shap_values,axis=1), linewidth=0, alpha=0.5, cmap=shap.plots.red_blue)\n",
    "cb = pl.colorbar(label=\"Model output\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "pl.gca().axis(\"off\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(5,5))\n",
    "pl.scatter(shap_embedded[:,0], shap_embedded[:,1], c=X_test[\"not\"] > 0, linewidth=0, alpha=0.5)\n",
    "cb = pl.colorbar(label=\"Log change in risk\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "pl.gca().axis(\"off\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(5,5))\n",
    "pl.scatter(shap_embedded[:,0], shap_embedded[:,1], c=X_test[\"never\"], linewidth=0, alpha=0.5)\n",
    "cb = pl.colorbar(label=\"Log change in risk\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "pl.gca().axis(\"off\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(5,5))\n",
    "pl.scatter(shap_embedded[:,0], shap_embedded[:,1], c=X_test[\"hour\"], linewidth=0, alpha=0.5)\n",
    "cb = pl.colorbar(label=\"Log change in risk\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "pl.gca().axis(\"off\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(5,5))\n",
    "pl.scatter(shap_embedded[:,0], shap_embedded[:,1], c=X_test[\"unit\"], linewidth=0, alpha=0.5)\n",
    "cb = pl.colorbar(label=\"Log change in risk\", aspect=40, orientation=\"horizontal\")\n",
    "cb.set_alpha(1)\n",
    "cb.draw_all()\n",
    "cb.outline.set_linewidth(0)\n",
    "cb.ax.tick_params('x', length=0)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "pl.gca().axis(\"off\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_orig = []\n",
    "preds_shap_mod = []\n",
    "preds_saabas_mod = []\n",
    "preds_gain_mod = []\n",
    "L = X_test.shape[0]\n",
    "shap_sums = [0]\n",
    "saabas_sums = [0]\n",
    "gain_sums = [0]\n",
    "for i in tqdm.tqdm(range(150)):\n",
    "    rand_ind = np.random.randint(X_test.shape[0])\n",
    "    tmp = X_test.iloc[i:i+1,:].copy()\n",
    "    #print()\n",
    "    preds_orig.append(model_train.predict(xgboost.DMatrix(tmp))[0])\n",
    "    \n",
    "    shap_values_tmp = model_train.predict(xgboost.DMatrix(tmp), pred_contribs=True)\n",
    "    ind = np.argmin(shap_values_tmp[0,:-1])\n",
    "    #print(X_test.columns[ind], \"==\", tmp.iloc[0,ind], \"-->\", shap_values_tmp[0,ind])\n",
    "#     if tmp.iloc[0,ind] == 0:\n",
    "#         tmp.iloc[0,ind] = 1\n",
    "#     else:\n",
    "#         tmp.iloc[0,ind] = 0\n",
    "    tmp.iloc[0,ind] = X_test.iloc[rand_ind,ind]\n",
    "    preds_shap_mod.append(model_train.predict(xgboost.DMatrix(tmp))[0])\n",
    "    #print(preds_shap_mod[-1]-preds_orig[-1])\n",
    "    shap_sums.append(preds_shap_mod[-1]-preds_orig[-1]+shap_sums[-1])\n",
    "    \n",
    "    tmp = X_test.iloc[i:i+1,:].copy()\n",
    "    saabas_values_tmp = model_train.predict(xgboost.DMatrix(tmp), pred_contribs=True, approx_contribs=True)\n",
    "    ind = np.argmin(saabas_values_tmp[0,:-1])\n",
    "    #print(X_test.columns[ind], \"==\", tmp.iloc[0,ind], \"-->\", saabas_values_tmp[0,ind])\n",
    "#     if tmp.iloc[0,ind] == 0:\n",
    "#         tmp.iloc[0,ind] = 1\n",
    "#     else:\n",
    "#         tmp.iloc[0,ind] = 0\n",
    "    tmp.iloc[0,ind] = X_test.iloc[rand_ind,ind]\n",
    "    preds_saabas_mod.append(model_train.predict(xgboost.DMatrix(tmp))[0])\n",
    "    #print(preds_saabas_mod[-1]-preds_orig[-1])\n",
    "    saabas_sums.append(preds_saabas_mod[-1]-preds_orig[-1]+saabas_sums[-1])\n",
    "    \n",
    "    tmp = X_test.iloc[i:i+1,:].copy()\n",
    "    ind = gain_ind\n",
    "#     if tmp.iloc[0,ind] == 0:\n",
    "#         tmp.iloc[0,ind] = 1\n",
    "#     else:\n",
    "#         tmp.iloc[0,ind] = 0\n",
    "    if tmp.iloc[0,ind] == 0:\n",
    "        tmp.iloc[0,ind] = X_test.iloc[rand_ind,ind]\n",
    "    preds_gain_mod.append(model_train.predict(xgboost.DMatrix(tmp))[0])\n",
    "    #print(preds_saabas_mod[-1]-preds_orig[-1])\n",
    "    gain_sums.append(preds_gain_mod[-1]-preds_orig[-1]+gain_sums[-1])\n",
    "    \n",
    "preds_orig = np.array(preds_orig)\n",
    "preds_shap_mod = np.array(preds_shap_mod)\n",
    "preds_saabas_mod = np.array(preds_saabas_mod)\n",
    "preds_gain_mod = np.array(preds_gain_mod)\n",
    "    #print(X_test.columns[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"thank\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction_values = model_train.predict(xgboost.DMatrix(X_test.iloc[:1000,:]), pred_interactions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_interaction_values, X_test.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "shap_interaction_values = model.predict(xgboost.DMatrix(X.iloc[:1000,:]), pred_interactions=True)\n",
    "time.time() - start"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
