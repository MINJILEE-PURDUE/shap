{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 6 - Supervised Clustering R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as pl\n",
    "import scipy.cluster\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import xgboost\n",
    "import sklearn.datasets\n",
    "import shap\n",
    "\n",
    "def plot_m(m, y, name=\"\", color=\"\"):\n",
    "    m = np.nan_to_num(m)\n",
    "    D = np.vstack([np.sum((m - m[i,:])**2, 1) for i in range(m.shape[0])])\n",
    "    clust = scipy.cluster.hierarchy.complete(D)\n",
    "\n",
    "    group_vals = [[y[i]] for i in range(m.shape[0])]\n",
    "    for i in range(len(clust)):\n",
    "        group_vals.append([])\n",
    "        #print(clust[i,0], clust[i,1])\n",
    "        group_vals[-1].extend(group_vals[int(clust[i,0])])\n",
    "        group_vals[-1].extend(group_vals[int(clust[i,1])])\n",
    "\n",
    "    count = m.shape[0]\n",
    "    counts = [count]\n",
    "    var = 1.0\n",
    "    variances = [var]\n",
    "    total_var = np.var(y)\n",
    "    for i in range(m.shape[0], len(group_vals)):\n",
    "        #print(np.var(group_vals[i]))\n",
    "        count = count - 1\n",
    "        counts.append(count)\n",
    "        clust_ind = i-m.shape[0]\n",
    "        ind1 = int(clust[clust_ind,0])\n",
    "        ind2 = int(clust[clust_ind,1])\n",
    "        var = var - np.var(group_vals[ind1])*len(group_vals[ind1])\n",
    "        var = var - np.var(group_vals[ind2])*len(group_vals[ind2])\n",
    "        var = var + np.var(group_vals[i])*(len(group_vals[ind1])+len(group_vals[ind2]))\n",
    "        variances.append(1-(var/total_var)/m.shape[0])\n",
    "    #print(variances)\n",
    "    #print(np.mean(variances), m.shape[0])\n",
    "\n",
    "    return pl.plot([x for x in counts], np.array(variances), color=color, linewidth=2, label=name+\" (AUC = \"+str(round(np.mean(variances),2))+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_expression = np.loadtxt(\"data/module_expression.txt\")\n",
    "cf = lambda x: -1000 if x == b'NA' else x\n",
    "neuropath = np.loadtxt(\"data/neuropath.txt\", converters={i:cf for i in range(8)})\n",
    "\n",
    "target = neuropath[:,1]\n",
    "dtrain = xgb.DMatrix(module_expression, label=target)\n",
    "dtrain = xgb.DMatrix(module_expression, label=target)\n",
    "param = { \"max_depth\": 6, \"base_score\": np.mean(target), \"eta\": 0.01}\n",
    "bst = xgb.train(param, dtrain, 300)\n",
    "out = bst.predict(xgb.DMatrix(module_expression), pred_contribs=True)\n",
    "out_path = bst.predict(xgb.DMatrix(module_expression), pred_contribs=True, approx_contribs=True)\n",
    "out_pred = bst.predict(xgb.DMatrix(module_expression))\n",
    "\n",
    "pl.close()\n",
    "pl.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plot_m(out, out_pred, \"SHAP\", color=\"#008BE0\")\n",
    "plot_m(out_path, out_pred, \"Path\", color=\"#ff165a\")\n",
    "#plot_m(module_expression, target, \"Unsupervised\", color=\"#18C45D\")\n",
    "pl.legend(loc=\"lower left\", frameon=False, prop={'size':10})\n",
    "pl.ylabel(\"R^2 (% variance explained)\")\n",
    "pl.xlabel(\"# groups\")\n",
    "pl.ylim(0,1)\n",
    "pl.xlim(0,len(target))\n",
    "pl.gca().invert_xaxis()\n",
    "#pl.figsize(5,4)\n",
    "#pl.figure(num=0, figsize=(4, 3))\n",
    "#pl.savefig(\"alz2.pdf\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "raw_train_data = np.genfromtxt(\"data/adult.data\", delimiter=\",\", dtype=None, autostrip=True, deletechars=[\"'\"])\n",
    "raw_test_data = np.genfromtxt(\"data/adult.test\", delimiter=\",\", dtype=None, autostrip=True, deletechars=[\"'\"], skip_header=1)\n",
    "\n",
    "# extract the category options in the training data\n",
    "col_names = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\",\n",
    "    \"capital-loss\", \"hours-per-week\", \"native-country\"\n",
    "]\n",
    "work_classes = list(set([v[col_names.index(\"workclass\")] for v in raw_train_data]))\n",
    "education_types = list(set([v[col_names.index(\"education\")] for v in raw_train_data]))\n",
    "marriage_statuses = list(set([v[col_names.index(\"marital-status\")] for v in raw_train_data]))\n",
    "occupations = list(set([v[col_names.index(\"occupation\")] for v in raw_train_data]))\n",
    "relationships = list(set([v[col_names.index(\"relationship\")] for v in raw_train_data]))\n",
    "races = list(set([v[col_names.index(\"race\")] for v in raw_train_data]))\n",
    "sexes = list(set([v[col_names.index(\"sex\")] for v in raw_train_data]))\n",
    "countries = list(set([v[col_names.index(\"native-country\")] for v in raw_train_data]))\n",
    "types = [work_classes, education_types, marriage_statuses, occupations, relationships, races, sexes, countries]\n",
    "N = raw_train_data.shape[0]\n",
    "P = sum(map(len, types)) + 5\n",
    "\n",
    "def build_matrix(data, P):\n",
    "    N = data.shape[0]\n",
    "    X = np.zeros((N, P))\n",
    "    \n",
    "    group_names = []\n",
    "    feature_groups = []\n",
    "    \n",
    "    \n",
    "    def assign_class(i, offset, name, classes, data_col):\n",
    "        if i == 0:\n",
    "            group_names.append(name)\n",
    "            feature_groups.append(list(range(offset, offset+len(classes))))\n",
    "        j = classes.index(data[i][data_col])\n",
    "        X[i,offset+j] = 1\n",
    "        offset += len(classes)\n",
    "        return offset\n",
    "    \n",
    "    def assign_num(i, offset, name, data_col):\n",
    "        if i == 0:\n",
    "            group_names.append(name)\n",
    "            feature_groups.append([offset])\n",
    "        X[i,offset] = data[i][data_col]\n",
    "        offset += 1\n",
    "        return offset\n",
    "    \n",
    "    for i in range(N):\n",
    "        offset = 0\n",
    "        offset = assign_num(i, offset, \"Age\", 0)\n",
    "        offset = assign_class(i, offset, \"Work class\", work_classes, 1)\n",
    "        offset = assign_class(i, offset, \"Education\", education_types, 3)\n",
    "        offset = assign_num(i, offset, \"Years in school\", 4)\n",
    "        offset = assign_class(i, offset, \"Marital status\", marriage_statuses, 5)\n",
    "        offset = assign_class(i, offset, \"Occupation\", occupations, 6)\n",
    "        offset = assign_class(i, offset, \"Relationship\", relationships, 7)\n",
    "        offset = assign_class(i, offset, \"Race\", races, 8)\n",
    "        offset = assign_class(i, offset, \"Sex\", sexes, 9)\n",
    "        offset = assign_num(i, offset, \"Capital gain\", 10)\n",
    "        offset = assign_num(i, offset, \"Capital loss\", 11)\n",
    "        offset = assign_num(i, offset, \"Weekly working hours\", 12)\n",
    "        offset = assign_class(i, offset, \"Native country\", countries, 13)\n",
    "        \n",
    "    y = np.array(list(v[-1] == b'>50K' for v in data))\n",
    "    \n",
    "    return X,y,group_names,feature_groups\n",
    "\n",
    "def group_values(x):\n",
    "    out = []\n",
    "    offset = 0\n",
    "    \n",
    "    def add_class(offset, class_members):\n",
    "        pos = -1\n",
    "        try:\n",
    "            pos = list(x[offset:offset+len(class_members)]).index(1)\n",
    "        except:\n",
    "            pass\n",
    "        out.append(\"\" if pos == -1 else class_members[pos])\n",
    "        offset += len(class_members)\n",
    "        return offset\n",
    "    \n",
    "    out.append(x[0])\n",
    "    offset += 1\n",
    "    offset = add_class(offset, work_classes)\n",
    "    offset = add_class(offset, education_types)\n",
    "    out.append(x[offset])\n",
    "    offset += 1\n",
    "    offset = add_class(offset, marriage_statuses)\n",
    "    offset = add_class(offset, occupations)\n",
    "    offset = add_class(offset, relationships)\n",
    "    offset = add_class(offset, races)\n",
    "    offset = add_class(offset, sexes)\n",
    "    out.append(x[offset])\n",
    "    offset += 1\n",
    "    out.append(x[offset])\n",
    "    offset += 1\n",
    "    out.append(x[offset])\n",
    "    offset += 1\n",
    "    offset = add_class(offset, countries)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# build the training data\n",
    "train_data,train_labels,group_names,feature_groups = build_matrix(raw_train_data, P)\n",
    "data_median = shap.DenseData(np.reshape(np.median(train_data,0), (1,train_data.shape[1])), group_names, feature_groups)\n",
    "\n",
    "# and test data\n",
    "test_data,test_labels,group_names,feature_groups = build_matrix(raw_test_data, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = list(range(train_data.shape[0]))\n",
    "random.shuffle(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_expression = train_data#np.loadtxt(\"data/module_expression.txt\")\n",
    "#cognitive_score = np.loadtxt(\"data/cognitive_score.txt\")\n",
    "#cf = lambda x: -1000 if x == b'NA' else x\n",
    "#neuropath = np.loadtxt(\"data/neuropath.txt\", converters={i:cf for i in range(8)})\n",
    "\n",
    "cut_ind = 31000\n",
    "\n",
    "target = train_labels#neuropath[:,label_ind]\n",
    "module_expression_train = module_expression[inds[:cut_ind],:]\n",
    "target_train = target[inds[:cut_ind]]\n",
    "module_expression_test = module_expression[inds[cut_ind:],:]\n",
    "target_test = target[inds[cut_ind:]]\n",
    "\n",
    "dtrain = xgb.DMatrix(module_expression_train, label=target_train)\n",
    "dtest = xgb.DMatrix(module_expression_test, label=target_test)\n",
    "param = { \"max_depth\": 6, \"base_score\": np.mean(target_train), \"eta\": 0.1, \"colsample_bytree\": 0.1}\n",
    "param = { \"max_depth\": 6, \"base_score\": np.mean(target_train), \"eta\": 0.1, \"subsample\": 0.5}\n",
    "bst = xgb.train(param, dtrain, 200)\n",
    "out = bst.predict(xgb.DMatrix(module_expression_test), pred_contribs=True)\n",
    "out_path = bst.predict(xgb.DMatrix(module_expression_test), pred_contribs=True, approx_contribs=True)\n",
    "pred = bst.predict(xgb.DMatrix(module_expression_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.close()\n",
    "pl.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plot_m(out, pred, \"SHAP\", color=\"#008BE0\")\n",
    "plot_m(out_path, pred, \"Path\", color=\"#ff165a\")\n",
    "#plot_m(module_expression_test_std, pred, \"Unsupervised\", color=\"#18C45D\")\n",
    "pl.legend(loc=\"lower left\", frameon=False, prop={'size':10})\n",
    "pl.ylabel(\"R^2 (% variance explained)\")\n",
    "pl.xlabel(\"# groups\")\n",
    "pl.ylim(0,1)\n",
    "pl.xlim(0,len(target_test))\n",
    "pl.gca().invert_xaxis()\n",
    "#pl.figsize(5,4)\n",
    "#pl.figure(num=0, figsize=(4, 3))\n",
    "#pl.savefig(\"census_data2.pdf\")\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
