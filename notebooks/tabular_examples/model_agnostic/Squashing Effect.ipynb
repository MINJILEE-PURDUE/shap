{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How a squashing function can effect feature importance\n",
    "\n",
    "The importance of a feature in a machine learning model can change significantly when you use a non-linear function to transform the model's output. The most common type of transformation where this matters is the use of a \"squashing\" function. Squashing functions such as the logistic transform are often used to convert an unbounded \"margin\" space to a bounded probability space. The value of the margin space is then in the units of information, while the values in the probability space is in the units of probability. Which space you care about can be different in different situations. The margin space is better for adding and subtracting, and directly corresponds to \"evidence\" in an information-theoretic sense. However, if you only care about changes in % probability, not evidence, then you would be better off using the probability space. By choosing probability space you are saying that getting lots of powerful evidence that takes you from 98% probability to 99.99% probability is not nearly as important as a smaller amount of evidence that takes you from 50% probability to 60% probability. Why does it take more evidence to go from 98% probability to 99.99% than from 50% probability to 60%? It is because in an information theoretic sense, it takes more information to go from 98% certainty to 99.99%, than it does to go from 50% certainty to 60%.\n",
    "\n",
    "Note that even though the logistic function is a monotonic transformation is can still change the ordering of which features are most important in a model. The ordering of features can change because some features may be very important for getting to 99.9% probability, while others are usually helpful in getting to 60% probability. The simple example below shows how you can change the importance of a feature using a squahing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost\n",
    "import scipy\n",
    "import shap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple dataset\n",
    "N = 500\n",
    "M = 4\n",
    "X = np.random.randn(N, M)\n",
    "X[0,0] = 0\n",
    "X[0,1] = 0\n",
    "X = pd.DataFrame(X, columns=[\"A\", \"B\", \"C\", \"D\"])\n",
    "# a function (a made up ML model) with an output in \"margin\" space...\n",
    "f = lambda X: (X[:,0] > 0) * 1 + (X[:,1] > 1.5) * 100\n",
    "\n",
    "# ...and then also change its output to probability space\n",
    "f_logistic = lambda X: scipy.special.expit(f(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain both functions\n",
    "explainer = shap.KernelExplainer(f, X)\n",
    "shap_values_f = explainer.shap_values(X.values[0:2,:])\n",
    "\n",
    "explainer_logistic = shap.KernelExplainer(f_logistic, X)\n",
    "shap_values_f_logistic = explainer_logistic.shap_values(X.values[0:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin space explaination\n",
    "\n",
    "When thinking about margin space, feature B is very important because by being 0 it means we don't hit the +100 effect that happens when B is greater than 2. Even though B being greater than 2 is rare, it is also very important because of the large impact it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_f[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(float(explainer.expected_value), shap_values_f[0,:], X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability space explaination\n",
    "\n",
    "When thinking about probability space, feature B is no longer very important because the logistic function squashes the effect of +100 in the margin space to just +1 at the most. So now feature B being larger than 2 is both rare and less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_f_logistic[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(float(explainer_logistic.expected_value), shap_values_f_logistic[0,:], X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
