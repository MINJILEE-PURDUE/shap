{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explain PyTorch MobileNetV2 using the `Partition` explainer\n",
    "\n",
    "In this example we are explaining the output of MobileNetV2 for classifying images into 1000 ImageNet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Loading Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.mobilenet_v2(pretrained=True, progress=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "X, y = shap.datasets.imagenet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ImageNet 1000 class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "with open(shap.datasets.cache(url)) as file:\n",
    "    class_names = [v[1] for v in json.load(file).values()]\n",
    "print(\"Number of ImageNet classes:\", len(class_names))\n",
    "#print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data transformation pipeline\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        x = x if x.shape[1] == 3 else x.permute(0, 3, 1, 2)\n",
    "    elif x.dim() == 3:\n",
    "        x = x if x.shape[0] == 3 else x.permute(2, 0, 1)\n",
    "    return x\n",
    "\n",
    "def nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        x = x if x.shape[3] == 3 else x.permute(0, 2, 3, 1)\n",
    "    elif x.dim() == 3:\n",
    "        x = x if x.shape[2] == 3 else x.permute(1, 2, 0)\n",
    "    return x \n",
    "        \n",
    "\n",
    "transform= [\n",
    "    torchvision.transforms.Lambda(nhwc_to_nchw),\n",
    "    torchvision.transforms.Lambda(lambda x: x*(1/255)),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std),\n",
    "    torchvision.transforms.Lambda(nchw_to_nhwc),\n",
    "]\n",
    "\n",
    "inv_transform= [\n",
    "    torchvision.transforms.Lambda(nhwc_to_nchw),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean = (-1 * np.array(mean) / np.array(std)).tolist(),\n",
    "        std = (1 / np.array(std)).tolist()\n",
    "    ),\n",
    "    torchvision.transforms.Lambda(nchw_to_nhwc),\n",
    "]\n",
    "\n",
    "transform = torchvision.transforms.Compose(transform)\n",
    "inv_transform = torchvision.transforms.Compose(inv_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img: np.ndarray) -> torch.Tensor:\n",
    "    img = nhwc_to_nchw(torch.Tensor(img))\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that transformations work correctly\n",
    "Xtr = transform(torch.Tensor(X))\n",
    "out = predict(Xtr[1:3])\n",
    "classes = torch.argmax(out, axis=1).cpu().numpy()\n",
    "print(f'Classes: {classes}: {np.array(class_names)[classes]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 4\n",
    "batch_size = 50\n",
    "n_evals = 10000\n",
    "\n",
    "# define a masker that is used to mask out partitions of the input image.\n",
    "masker_blur = shap.maskers.Image(\"blur(128,128)\", Xtr[0].shape)\n",
    "\n",
    "# create an explainer with model and image masker\n",
    "explainer = shap.Explainer(predict, masker_blur, output_names=class_names)\n",
    "\n",
    "# feed only one image\n",
    "# here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(Xtr[1:2], max_evals=n_evals, batch_size=batch_size,\n",
    "                        outputs=shap.Explanation.argsort.flip[:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(shap_values.data.shape, shap_values.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.data = inv_transform(shap_values.data).cpu().numpy()[0]\n",
    "shap_values.values = [val for val in np.moveaxis(shap_values.values[0],-1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values=shap_values.values, \n",
    "                pixel_values=shap_values.data, \n",
    "                labels=shap_values.output_names,\n",
    "                true_labels=[class_names[132]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a masker that is used to mask out partitions of the input image.\n",
    "masker_blur = shap.maskers.Image(\"blur(128,128)\", Xtr[0].shape)\n",
    "\n",
    "# create an explainer with model and image masker\n",
    "explainer = shap.Explainer(predict, masker_blur, output_names=class_names)\n",
    "\n",
    "# feed only one image\n",
    "# here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(Xtr[1:4], max_evals=n_evals, batch_size=batch_size,\n",
    "                        outputs=shap.Explanation.argsort.flip[:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shap_values.data.shape, shap_values.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.data = inv_transform(shap_values.data).cpu().numpy()\n",
    "shap_values.values = [val for val in np.moveaxis(shap_values.values,-1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shap_values.data.shape, shap_values.values[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values=shap_values.values,\n",
    "                pixel_values=shap_values.data,\n",
    "                labels=shap_values.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
