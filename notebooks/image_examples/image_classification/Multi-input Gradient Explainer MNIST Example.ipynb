{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-input Gradient Explainer MNIST Example\n",
    "\n",
    "Here we demonstrate how to use GradientExplainer when you have multiple inputs to your Keras/TensorFlow model. To keep things simple but also mildly interesting we feed two copies of MNIST into our model, where one copy goes into a conv-net layer and the other copy goes directly into a feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D\n",
    "\n",
    "# load the MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# define our model\n",
    "input1 = Input(shape=(28,28,1))\n",
    "input2 = Input(shape=(28,28,1))\n",
    "input2c = Conv2D(32, kernel_size=(3, 3), activation='relu')(input2)\n",
    "joint = tf.keras.layers.concatenate([Flatten()(input1), Flatten()(input2c)])\n",
    "out = Dense(10, activation='softmax')(Dropout(0.2)(Dense(128, activation='relu')(joint)))\n",
    "model = tf.keras.models.Model(inputs = [input1, input2], outputs=out)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit([x_train, x_train], y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the predictions made by the model using GradientExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# since we have two inputs we pass a list of inputs to the explainer\n",
    "explainer = shap.GradientExplainer(model, [x_train, x_train])\n",
    "\n",
    "# we explain the model's predictions on the first three samples of the test set\n",
    "shap_values = explainer.shap_values([x_test[:3], x_test[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the model has 10 outputs we get a list of 10 explanations (one for each output)\n",
    "print(len(shap_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the model has 2 inputs we get a list of 2 explanations (one for each input) for each output\n",
    "print(len(shap_values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot the explanations for all classes for the first input (this is the feed forward input)\n",
    "shap.image_plot([shap_values[i][0] for i in range(10)], x_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot the explanations for all classes for the second input (this is the conv-net input)\n",
    "shap.image_plot([shap_values[i][1] for i in range(10)], x_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the sampling error\n",
    "\n",
    "By setting `return_variances=True` we get an estimate of how accurate our explanations are. We can see that the default number of samples (200) that were used provide fairly low variance estimates (compared to the magnitude of the shap_values above). Note that you can always use the `nsamples` parameter to control how many samples are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the variance of our estimates\n",
    "shap_values, shap_values_var = explainer.shap_values([x_test[:3], x_test[:3]], return_variances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot the explanations for all classes for the first input (this is the feed forward input)\n",
    "shap.image_plot([shap_values_var[i][0] for i in range(10)], x_test[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
