{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark XGBoost explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares several different explantion methods when applied to XGBoost models. These methods are compared across many different evaluation metrics. Explanation error is the primary metric we sort by, but we also compare across many other metrics, since no single metric fully captures the performance of an attribution explanation method.\n",
    "\n",
    "For a more detailed explanation of each of the metrics used here please check out the documation strings of the various classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark explainers on an XGBoost regression model of Boston housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "import shap\n",
    "import shap.benchmark\n",
    "\n",
    "# build the model\n",
    "model = xgboost.XGBRegressor(n_estimators=1000, subsample=0.3)\n",
    "X,y = shap.datasets.boston()\n",
    "X = X.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "# define the benchmark evaluation sample set\n",
    "X_eval = X_test[:]\n",
    "y_eval = y_test[:]\n",
    "\n",
    "# use an independent masker\n",
    "masker = shap.maskers.Independent(X_train)\n",
    "pmasker = shap.maskers.Partition(X_train)\n",
    "\n",
    "# build the explainers\n",
    "explainers = [\n",
    "    (\"Permutation\", shap.explainers.Permutation(model.predict, masker)),\n",
    "    (\"Permutation part.\", shap.explainers.Permutation(model.predict, pmasker)),\n",
    "    (\"Partition\", shap.explainers.Partition(model.predict, pmasker)),\n",
    "    (\"Tree\", shap.explainers.Tree(model, masker)),\n",
    "    (\"Tree approx.\", shap.explainers.Tree(model, masker, approximate=True)),\n",
    "    (\"Exact\", shap.explainers.Exact(model.predict, masker)),\n",
    "    (\"Random\", shap.explainers.other.Random(model.predict, masker))\n",
    "]\n",
    "\n",
    "# # dry run to get all the code warmed up for valid runtime measurements\n",
    "for name, exp in explainers:\n",
    "    exp(X_eval[:1])\n",
    "\n",
    "# explain with all the explainers\n",
    "attributions = [(name, exp(X_eval)) for name, exp in explainers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "smasker = shap.benchmark.ExplanationError(\n",
    "    masker, model.predict, X_eval\n",
    ")\n",
    "results[\"explanation error\"] = [smasker(v, name=n) for n,v in attributions]\n",
    "\n",
    "ct = shap.benchmark.ComputeTime()\n",
    "results[\"compute time\"] = [ct(v, name=n) for n,v in attributions]\n",
    "\n",
    "for mask_type, ordering in [(\"keep\", \"positive\"), (\"remove\", \"positive\"), (\"keep\", \"negative\"), (\"remove\", \"negative\")]:\n",
    "    smasker = shap.benchmark.SequentialMasker(\n",
    "        mask_type, ordering, masker, model.predict, X_eval\n",
    "    )\n",
    "    results[mask_type + \" \" + ordering] = [smasker(v, name=n) for n,v in attributions]\n",
    "\n",
    "cmasker = shap.maskers.Composite(masker, shap.maskers.Fixed())\n",
    "for mask_type, ordering in [(\"keep\", \"absolute\"), (\"remove\", \"absolute\")]:\n",
    "    smasker = shap.benchmark.SequentialMasker(\n",
    "        mask_type, ordering, cmasker, lambda X, y: (y - model.predict(X))**2, X_eval, y_eval\n",
    "    )\n",
    "    results[mask_type + \" \" + ordering] = [smasker(v, name=n) for n,v in attributions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show scores across all metrics for all explainers\n",
    "\n",
    "This multi-metric benchmark plot sorts the method by the first method, and rescales the scores to be relative for each metric, so that the best score appears at the top and the worse score at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.benchmark(sum(results.values(), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show overall performance again but without Random\n",
    "\n",
    "Since random scores are so much worse than reasonable explanation methods, we draw the same plot again but without the Random method so we can see smaller variations in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.benchmark(filter(lambda x: x.method != \"Random\", sum(results.values(), [])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show detail plots of each metric type\n",
    "\n",
    "If we plot scores for one metric at a time then we can see a much more detailed comparison of the methods. Some methods just have a score (explanation error and compute time) while other methods have entire performance curves, and the score is the area under (or over) these curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_plot_rows = len(results) // 2 + len(results) % 2\n",
    "fig, ax = plt.subplots(num_plot_rows, 2, figsize=(12, 5 * num_plot_rows))\n",
    "\n",
    "for i, k in enumerate(results):\n",
    "    plt.subplot(num_plot_rows, 2, i+1)\n",
    "    shap.plots.benchmark(results[k], show=False)\n",
    "if i % 2 == 0:\n",
    "    ax[-1, -1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark explainers on an XGBoost classification model of census reported income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = xgboost.XGBClassifier(n_estimators=1000, subsample=0.3)\n",
    "X,y = shap.datasets.adult()\n",
    "X = X.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n",
    "def logit_predict(X):\n",
    "    return model.predict(X, output_margin=True)\n",
    "def loss_predict(X, y):\n",
    "    probs = model.predict_proba(X)\n",
    "    return [-np.log(probs[i,y[i]*1]) for i in range(len(y))]\n",
    "\n",
    "\n",
    "# define the benchmark evaluation sample set (limited to 1000 samples for the sake of time)\n",
    "X_eval = X_test[:1000]\n",
    "y_eval = y_test[:1000]\n",
    "\n",
    "# use an independent masker\n",
    "masker = shap.maskers.Independent(X_train)\n",
    "pmasker = shap.maskers.Partition(X_train)\n",
    "\n",
    "# build the explainers\n",
    "explainers = [\n",
    "    (\"Permutation\", shap.explainers.Permutation(logit_predict, masker)),\n",
    "    (\"Permutation part.\", shap.explainers.Permutation(logit_predict, pmasker)),\n",
    "    (\"Partition\", shap.explainers.Partition(logit_predict, pmasker)),\n",
    "    (\"Tree\", shap.explainers.Tree(model, masker)),\n",
    "    (\"Tree approx.\", shap.explainers.Tree(model, masker, approximate=True)),\n",
    "    (\"Random\", shap.explainers.other.Random(logit_predict, masker)),\n",
    "    (\"Exact\", shap.explainers.Exact(logit_predict, masker))\n",
    "]\n",
    "\n",
    "# # dry run to get all the code warmed up for valid runtime measurements\n",
    "for name, exp in explainers:\n",
    "    exp(X_eval[:1])\n",
    "\n",
    "# explain with all the explainers\n",
    "attributions = [(name, exp(X_eval)) for name, exp in explainers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# we run explanation error first as the primary metric\n",
    "smasker = shap.benchmark.ExplanationError(\n",
    "    masker, logit_predict, X_eval\n",
    ")\n",
    "results[\"explanation error\"] = [smasker(v, name=n) for n,v in attributions]\n",
    "\n",
    "# next compute time\n",
    "ct = shap.benchmark.ComputeTime()\n",
    "results[\"compute time\"] = [ct(v, name=n) for n,v in attributions]\n",
    "\n",
    "# then removal and addition of feature metrics based on model output\n",
    "for mask_type, ordering in [(\"keep\", \"positive\"), (\"remove\", \"positive\"), (\"keep\", \"negative\"), (\"remove\", \"negative\")]:\n",
    "    smasker = shap.benchmark.SequentialMasker(\n",
    "        mask_type, ordering, masker, logit_predict, X_eval\n",
    "    )\n",
    "    results[mask_type + \" \" + ordering] = [smasker(v, name=n) for n,v in attributions]\n",
    "\n",
    "# then removal and addition of feature metrics based on model loss\n",
    "cmasker = shap.maskers.Composite(masker, shap.maskers.Fixed())\n",
    "for mask_type, ordering in [(\"keep\", \"absolute\"), (\"remove\", \"absolute\")]:\n",
    "    smasker = shap.benchmark.SequentialMasker(\n",
    "        mask_type, ordering, cmasker, loss_predict, X_eval, y_eval\n",
    "    )\n",
    "    results[mask_type + \" \" + ordering] = [smasker(v, name=n) for n,v in attributions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an overall area-under-curve score across all metrics for all explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.benchmark(sum(results.values(), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show overall performance again but without Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.benchmark(filter(lambda x: x.method != \"Random\", sum(results.values(), [])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show detail plots of each metric type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_plot_rows = len(results) // 2 + len(results) % 2\n",
    "fig, ax = plt.subplots(num_plot_rows, 2, figsize=(12, 5 * num_plot_rows))\n",
    "\n",
    "for i, k in enumerate(results):\n",
    "    plt.subplot(num_plot_rows, 2, i+1)\n",
    "    shap.plots.benchmark(results[k], show=False)\n",
    "if i % 2 == 0:\n",
    "    ax[-1, -1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
